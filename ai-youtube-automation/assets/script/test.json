{
  "text":"Have you ever wanted to give an AI agent access to your own private research documents without uploading everything to the cloud or paying huge API fees? Imagine typing a question into Claude Desktop, but instead of just relying on its training data, it instantly pulls answers from your local database running on your own machine. That is exactly what we are building today. We are going to create a custom Model Context Protocol, or MCP server, using FastAPI, connect it to a local Qdrant vector database, and power the whole thing with Ollama. This is a massive step forward for anyone interested in deep research automation and local AI privacy. Welcome back to Qubot AI Automation. If you are a busy creator or developer looking to streamline your workflow, you are in the right place. Today, we are tackling a Proof of Concept that connects several powerful tools. We are moving beyond simple scripts and building a robust architecture that acts as the brain for your automation projects. I know a lot of you have been hearing about MCP servers recently. It feels like the new standard for connecting AI tools, and frankly, it is. But the documentation can be a bit heavy. So, I spent the last few days banging my head against the wall to figure out the simplest way to get this running, so you don't have to. Let's start with the Why. Why build this specific stack? We have FastAPI as our backbone. It is fast, lightweight, and perfect for building the endpoints that the MCP protocol needs. Then we have Ollama. This allows us to run models like Llama 3 or Mistral locally. This is crucial for embedding our data—transforming text into numbers that the computer understands—without paying OpenAI for embeddings. Finally, we use Qdrant. Qdrant is a fantastic vector search engine. It stores those embeddings and lets us find relevant information in milliseconds. By combining these, you get a research agent that lives entirely on your laptop Here is the first hurdle I faced during this research. When you look at the MCP documentation, most examples use standard input and output, or stdio. That works great for simple command-line tools. But if you want to integrate this with n8n later, or if you want to debug what is actually happening, running an HTTP server with Server-Sent Events, or SSE, is much better. So, that is the route we are taking today. We are building an SSE-based MCP server",
  "service_model": "TTS",
  "speaker_id": "deepakVoice01",
  "language": "en",
  "out_path": "C:/Users/ermdi/projects/ird-projects/de-ds-ai-automation/ai-youtube-automation/output/clone_voice/deepak_en.wav"
}