{
  "script_text": "Have you ever wanted to give an AI agent access to your own private research documents without uploading everything to the cloud or paying huge API fees? Imagine typing a question into Claude Desktop, but instead of just relying on its training data, it instantly pulls answers from your local database running on your own machine. That is exactly what we are building today. We are going to create a custom Model Context Protocol, or MCP server, using FastAPI, connect it to a local Qdrant vector database, and power the whole thing with Ollama. This is a massive step forward for anyone interested in deep research automation and local AI privacy.\n\nWelcome back to Qubot AI Automation. If you are a busy creator or developer looking to streamline your workflow, you are in the right place. Today, we are tackling a Proof of Concept that connects several powerful tools. We are moving beyond simple scripts and building a robust architecture that acts as the brain for your automation projects. I know a lot of you have been hearing about MCP servers recently. It feels like the new standard for connecting AI tools, and frankly, it is. But the documentation can be a bit heavy. So, I spent the last few days banging my head against the wall to figure out the simplest way to get this running, so you don't have to.\n\nLet's start with the \"Why\". Why build this specific stack? We have FastAPI as our backbone. It is fast, lightweight, and perfect for building the endpoints that the MCP protocol needs. Then we have Ollama. This allows us to run models like Llama 3 or Mistral locally. This is crucial for embedding our data—transforming text into numbers that the computer understands—without paying OpenAI for embeddings. Finally, we use Qdrant. Qdrant is a fantastic vector search engine. It stores those embeddings and lets us find relevant information in milliseconds. By combining these, you get a research agent that lives entirely on your laptop.\n\nHere is the first hurdle I faced during this research. When you look at the MCP documentation, most examples use standard input and output, or stdio. That works great for simple command-line tools. But if you want to integrate this with n8n later, or if you want to debug what is actually happening, running an HTTP server with Server-Sent Events, or SSE, is much better. So, that is the route we are taking today. We are building an SSE-based MCP server.\n\nLet's walk through the setup. First, you need Docker running. I like to run Qdrant in a Docker container because it keeps my system clean. You will pull the Qdrant image and start it on port 6333. A common mistake here is forgetting to map the storage volume. If you don't map a volume, every time you restart Docker, your research database disappears. So, make sure you mount a local folder to the container. Once Qdrant is up, you can go to your browser at localhost 6333 to see the dashboard. It is empty now, but not for long.\n\nNext, let's look at Ollama. You need to have Ollama installed and running. For this POC, we are not just using it for chat; we are using it for embeddings. I recommend pulling the 'nomic-embed-text' model. It is small, fast, and highly accurate for retrieval tasks. In your terminal, just type 'ollama pull nomic-embed-text'. If you skip this and try to use a generic chat model for embeddings, your search results in Qdrant will look weird. I learned that the hard way. Chat models generate text; embedding models generate vectors. Keep them separate.\n\nNow, the code. We are writing a Python script using FastAPI. We need to import the MCP SDK. The core concept here is defining a \"Tool\". In the MCP world, a Tool is just a function that the AI can call. We are going to define a tool called \"search_knowledge_base\". When Claude decides it needs information, it will call this tool with a query string.\n\nHere is where the magic happens, and where the logic can get tricky. Inside our search function, we take the user's query and send it to Ollama. Ollama gives us back a vector—a list of floating-point numbers. Then, we take that vector and send it to Qdrant using the Qdrant client. Qdrant scans our collection and returns the top three or five most relevant chunks of text. Finally, we format that text and return it to the MCP server, which sends it back to Claude. It sounds like a lot of steps, but in Python, it is only about fifty lines of code.\n\nOne specific pain point I want to highlight involves the networking. If you are running your FastAPI server on your host machine, and Qdrant in Docker, you generally use 'localhost' to connect. However, if you decide later to put your FastAPI server inside a Docker container as well, 'localhost' will stop working because 'localhost' inside a container refers to the container itself, not your computer. For this POC, to keep it simple, run the Python script directly on your machine. It saves you a lot of headache with Docker networking bridges.\n\nLet's talk about populating the data. A search engine is useless without data. This is where workflow automation comes in. You could write a script to load files, but I prefer using n8n. I set up a simple n8n workflow that watches a Google Drive folder. When I drop a PDF in there, n8n parses the text, sends it to Ollama to get the embeddings, and pushes it into Qdrant. This way, my research agent is always up to date. I am not manually coding the data ingestion every time. This separation is key: n8n handles the input, and our FastAPI MCP server handles the retrieval.\n\nOnce the code is written, we launch the server using Uvicorn. You should see it starting up on port 8000. But we are not done. The final step is telling Claude Desktop about this new tool. You need to find your Claude configuration file. On a Mac, it is in the Application Support folder. On Windows, it is in AppData. You need to edit this JSON file to add your new server. Since we are using SSE, you provide the URL of your FastAPI server. Be very careful with the JSON syntax here. A missing comma will prevent Claude from loading any tools at all.\n\nAfter you save the config and restart Claude, look for the little hammer icon. If you see it, congratulations! Your local MCP server is active. You can now type a prompt like, \"Based on my research database, what were the key findings from last week's project?\" You will see Claude explicitly call the \"search_knowledge_base\" tool, wait for your local FastAPI server to process the vector search in Qdrant, and then generate an answer based on your private data.\n\nThis architecture is powerful because it is modular. Today we used Qdrant, but you could swap it for ChromaDB or Postgres. We used Ollama, but you could swap it for a different local LLM provider. The MCP standard acts as the glue. It allows us to build these complex, privacy-focused research tools without reinventing the wheel every time.\n\nI really believe that for solo developers and small business owners, owning your data infrastructure is going to be critical. We rely too much on cloud APIs that can change their pricing or privacy policies overnight. By building a local POC like this, you ensure that your research workflow remains yours.\n\nIf you found this breakdown helpful, please hit that like button—it really helps the channel grow and reach more automation enthusiasts like you. And make sure to subscribe because in the next video, I am going to show you how to connect this exact MCP server to a live web-scraping agent. If you are stuck on the Qdrant setup or the FastAPI routing, drop a comment below. I try to answer as many technical questions as I can. Thanks for watching, and happy automating.",
  "title": "Build a Local MCP Server with FastAPI, Ollama & Qdrant",
  "description": "Learn how to build a private AI research agent using the Model Context Protocol (MCP). In this deep-dive tutorial, we create a custom MCP server using FastAPI, connect it to a local Qdrant vector database, and use Ollama for local embeddings. This setup allows tools like Claude Desktop to chat with your private documents securely, with zero API costs. Perfect for developers and creators who want to own their AI workflow.",
  "tags": [
    "mcp server",
    "fastapi",
    "ollama",
    "qdrant",
    "ai automation",
    "workflow automation",
    "local ai",
    "vector database",
    "python tutorial",
    "research agent"
  ]
}